---
title: Thinking, Fast and Slow
---

![](https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1317793965l/11468377.jpg)

# Thinking, Fast and Slow (Kahneman, Daniel)

> “The situation has provided a cue; this cue has given the expert access to
> information stored in memory, and the information provides the answer.
> Intuition is nothing more and nothing less than recognition.”

> Overconfidence is fed by the illusory certainty of hindsight. My views on this
> topic have been influenced by Nassim Taleb, the author of The Black Swan. I
> hope for watercooler conversations that intelligently explore the lessons that
> can be learned from the past while resisting the lure of hindsight and the
> illusion of certainty.

> The gorilla study illustrates two important facts about our minds: we can be
> blind to the obvious, and we are also blind to our blindness.

> you know that the lines are equally long. If asked about their length, you
> will say what you know. But you still see the bottom line as longer. You have
> chosen to believe the measurement, but you cannot prevent System 1 from doing
> its thing; you cannot decide to see the lines as equal, although you know they
> are.

> Not all illusions are visual. There are illusions of thought, which we call
> cognitive illusions.

> As a way to live your life, however, continuous vigilance is not necessarily
> good, and it is certainly impractical. Constantly questioning our own thinking
> would be impossibly tedious, and System 2 is much too slow and inefficient to
> serve as a substitute for System 1 in making routine decisions. The best we
> can do is a compromise: learn to recognize situations in which mistakes are
> likely and try harder to avoid significant mistakes when the stakes are high.
> The premise of this book is that it is easier to recognize other people’s
> mistakes than our own.

> the pupil was a good measure of the physical arousal that accompanies mental
> effort, and we could go ahead and use it to understand how the mind works.
> Much like the electricity meter outside your house or apartment, the pupils
> offer an index of the current rate at which mental energy is used.

> Any task that requires you to keep several ideas in mind at the same time has
> the same hurried character. Unless you have the good fortune of a capacious
> working memory, you may be forced to work uncomfortably hard. The most
> effortful forms of slow thinking are those that require you to think fast.

> The psychologist Mihaly Csikszentmihalyi (pronounced six-cent-mihaly) has done
> more than anyone else to study this state of effortless attending, and the
> name he proposed for it, flow, has become part of the language. People who
> experience flow describe it as “a state of effortless concentration so deep
> that they lose their sense of time, of themselves, of their problems,” and
> their descriptions of the joy of that state are so compelling that
> Csikszentmihalyi has called it an “optimal experience.”

> People who are cognitively busy are also more likely to make selfish choices,
> use sexist language, and make superficial judgments in social situations.

> The nervous system consumes more glucose than most other parts of the body,
> and effortful mental activity appears to be especially expensive in the
> currency of glucose. When you are actively involved in difficult cognitive
> reasoning or engaged in a task that requires self-control, your blood glucose
> level drops. The effect is analogous to a runner who draws down glucose stored
> in her muscles during a sprint. The bold implication of this idea is that the
> effects of ego depletion could be undone by ingesting glucose,

> if you have had to force yourself to do something, you are less willing or
> less able to exert self-control when the next challenge comes around. The
> phenomenon has been named ego depletion.

> when people believe a conclusion is true, they are also very likely to believe
> arguments that appear to support it, even when these arguments are unsound. If
> System 1 is involved, the conclusion comes first and the arguments follow.

> Intelligence is not only the ability to reason; it is also the ability to find
> relevant material in memory and to deploy attention when needed.

> Those who avoid the sin of intellectual sloth could be called “engaged.” They
> are more alert, more intellectually active, less willing to be satisfied with
> superficially attractive answers, more skeptical about their intuitions. The
> psychologist Keith Stanovich would call them more rational.

> As cognitive scientists have emphasized in recent years, cognition is
> embodied; you think with your body, not only with your brain.

> This remarkable priming phenomenon—the influencing of an action by the idea—is
> known as the ideomotor effect.

> being amused tends to make you smile, and smiling tends to make you feel
> amused.

> “act calm and kind regardless of how you feel” is very good advice: you are
> likely to be rewarded by actually feeling calm and kind.

> reliable way to make people believe in falsehoods is frequent repetition,
> because familiarity is not easily distinguished from truth.

> All this is very good advice, but we should not get carried away. High-quality
> paper, bright colors, and rhyming or simple language will not be much help if
> your message is obviously nonsensical, or if it contradicts facts that your
> audience knows to be true.

> performance was better with the bad font.

> Zajonc argued that the effect of repetition on liking is a profoundly
> important biological fact, and that it extends to all animals. To survive in a
> frequently dangerous world, an organism should react cautiously to a novel
> stimulus, with withdrawal and fear. Survival prospects are poor for an animal
> that is not suspicious of novelty. However, it is also adaptive for the
> initial caution to fade if the stimulus is actually safe. The mere exposure
> effect occurs, Zajonc claimed, because the repeated exposure of a stimulus is
> followed by nothing bad. Such a stimulus will eventually become a safety
> signal, and safety is good.

> Mood evidently affects the operation of System 1: when we are uncomfortable
> and unhappy, we lose touch with our intuition.

> the connection makes biological sense. A good mood is a signal that things are
> generally going well, the environment is safe, and it is all right to let
> one’s guard down. A bad mood indicates that things are not going very well,
> there may be a threat, and vigilance is required. Cognitive ease is both a
> cause and a consequence of a pleasant feeling.

> We have norms for a vast number of categories, and these norms provide the
> background for the immediate detection of anomalies

> a large event is supposed to have consequences, and consequences need causes
> to explain them. We have limited information about what happened on a day, and
> System 1 is adept at finding a coherent causal story that links the fragments
> of knowledge at its disposal.

> we are born prepared to make intentional attributions: infants under one year
> old identify bullies and victims, and expect a pursuer to follow the most
> direct path in attempting to catch whatever it is chasing.

> when System 2 is otherwise engaged, we will believe almost anything. System 1
> is gullible and biased to believe, System 2 is in charge of doubting and
> unbelieving, but System 2 is sometimes busy, and often lazy. Indeed, there is
> evidence that people are more likely to be influenced by empty persuasive
> messages, such as commercials, when they are tired and depleted.

> The tendency to like (or dislike) everything about a person—including things
> you have not observed—is known as the halo effect.

> Gilbert proposed that understanding a statement must begin with an attempt to
> believe it: you must first know what the idea would mean if it were true. Only
> then can you decide whether or not to unbelieve it.

> simple rule can help: before an issue is discussed, all members of the committee
> should be asked to write a very brief summary of their position. This procedure
> makes good use of the value of the diversity of knowledge and opinion in the
> group.

> Jumping to conclusions on the basis of limited evidence is so important to an
> understanding of intuitive thinking, and comes up so often in this book, that
> I will use a cumbersome abbreviation for it: WYSIATI, which stands for what
> you see is all there is. System 1 is radically insensitive to both the quality
> and the quantity of the information that gives rise to impressions and
> intuitions.

> It is the consistency of the information that matters for a good story, not
> its completeness. Indeed, you will often find that knowing little makes it
> easier to fit everything you know into a coherent pattern.

> Here we encounter a new aptitude of System 1. An underlying scale of intensity
> allows matching across diverse dimensions.

> assessment. You do that automatically whether or not you

> you often have answers to questions that you do not completely understand,
> relying on evidence that you can neither explain nor defend.

> How to Solve It: “If you can’t solve a problem, then there is an easier
> problem you can solve: find it.” Pólya’s heuristics are strategic procedures
> that are deliberately implemented by System 2.

> The dominance of conclusions over arguments is most pronounced where emotions
> are involved.

> “people are not adequately sensitive to sample size.”

> we are prone to exaggerate the consistency and coherence of what we see. The
> exaggerated faith of researchers in what can be learned from a few
> observations is closely related to the halo effect, the sense we often get
> that we know and understand a person about whom we actually know very little.

> if you follow your intuition, you will more often than not err by
> misclassifying a random event as systematic. We are far too willing to reject
> the belief that much of what we see in life is random.

> anchoring effect. It occurs when people consider a particular value for an
> unknown quantity before estimating that quantity. What happens is one of the
> most reliable and robust results of experimental psychology: the estimates
> stay close to the number that people considered—hence the image of an anchor.

> The conclusion is clear: anchors do not have their effects because people
> believe they are informative.

> My advice to students when I taught negotiations was that if you think the
> other side has made an outrageous proposal, you should not come back with an
> equally outrageous counteroffer, creating a gap that will be difficult to
> bridge in further negotiations. Instead you should make a scene, storm out or
> threaten to do so, and make it clear—to yourself as well as to the other
> side—that you will not continue the negotiation with that number on the table.

> general, a strategy of deliberately “thinking the opposite” may be a good
> defense against anchoring effects, because it negates the biased recruitment
> of thoughts that produces these effects.

> You are always aware of the anchor and even pay attention to it, but you do
> not know how it guides and constrains your thinking, because you cannot
> imagine how you would have thought if the anchor had been different (or
> absent). However, you should assume that any number that is on the table has
> had an anchoring effect on you, and if the stakes are high you should mobilize
> yourself (your System 2) to combat the effect.

> Kunreuther also observed that protective actions, whether by individuals or
> governments, are usually designed to be adequate to the worst disaster
> actually experienced.

> The world in our heads is not a precise replica of reality; our expectations
> about the frequency of events are distorted by the prevalence and emotional
> intensity of the messages to which we are exposed.

> Slovic has challenged the foundation of their expertise: the idea that risk is
> objective. “Risk” does not exist “out there,” independent of our minds and
> culture, waiting to be measured. Human beings have invented the concept of
> “risk” to help them understand and cope with the dangers and uncertainties of
> life. Although these dangers are real, there is no such thing as “real risk”
> or “objective risk.”

> Every parent who has stayed up waiting for a teenage daughter who is late from
> a party will recognize the feeling. You may know that there is really (almost)
> nothing to worry about, but you cannot help images of disaster from coming to
> mind. As Slovic has argued, the amount of concern is not adequately sensitive
> to the probability of harm; you are imagining the numerator—the tragic story
> you saw on the news—and not thinking about the denominator. Sunstein has
> coined the phrase “probability neglect” to describe the pattern. The
> combination of probability neglect with the social mechanisms of availability
> cascades inevitably leads to gross exaggeration of minor threats, sometimes
> with important consequences.

> mechanism through which biases flow into policy: the availability cascade.

> Anchor your judgment of the probability of an outcome on a plausible base
> rate. Question the diagnosticity of your evidence.

> individuals feel relieved of responsibility when they know that others have
> heard the same request for help.

> Changing one’s mind about human nature is hard work, and changing one’s mind
> for the worse about oneself is even harder.

> Subjects’ unwillingness to deduce the particular from the general was matched
> only by their willingness to infer the general from the particular.

> You are more likely to learn something by finding surprises in your own
> behavior than by hearing surprising facts about people in general.

> rewards for improved performance work better than punishment of mistakes.

> correlation and regression are not two concepts—they are different
> perspectives on the same concept. The general rule is straightforward but has
> surprising consequences: whenever the correlation between two scores is
> imperfect, there will be regression to the mean. To

> In order to conclude that an energy drink—or any other treatment—is effective,
> you must compare a group of patients who receive this treatment to a “control
> group” that receives no treatment (or, better, receives a placebo).

> general limitation of the human mind is its imperfect ability to reconstruct
> past states of knowledge, or beliefs that have changed. Once you adopt a new
> view of the world (or of any part of it), you immediately lose much of your
> ability to recall what you used to believe before your mind changed.

> In other words, people who spend their time, and earn their living, studying a
> particular topic produce poorer predictions than dart-throwing monkeys who
> would have distributed their choices evenly over the options. Even in the
> region they knew best, experts were not significantly better than
> nonspecialists. Those who know more forecast very slightly better than those
> who know less. But those with the most knowledge are often less reliable. The
> reason is that the person who acquires more knowledge develops an enhanced
> illusion of her skill and becomes unrealistically overconfident.

> The first lesson is that errors of prediction are inevitable because the world
> is unpredictable. The second is that high subjective confidence is not to be
> trusted as an indicator of accuracy (low confidence could be more
> informative).

> Several studies have shown that human decision makers are inferior to a
> prediction formula even when they are given the score suggested by the
> formula!

> “The situation has provided a cue; this cue has given the expert access to
> information stored in memory, and the information provides the answer.
> Intuition is nothing more and nothing less than recognition.”

> intuition cannot be trusted in the absence of stable regularities in the
> environment.

> when a project ends reasonably well: once you understand the main conclusion,
> it seems it was always obvious.

> how can we evaluate the probable validity of an intuitive judgment? When do
> judgments reflect true expertise? When do they display an illusion of
> validity? The answer comes from the two basic conditions for acquiring a
> skill: an environment that is sufficiently regular to be predictable an
> opportunity to learn these regularities through prolonged practice

> the likelihood that something will go wrong in a big project is high.

> people who have information about an individual case rarely feel the need to
> know the statistics of the class to which the case belongs. When we were
> eventually exposed to the outside view, we collectively ignored it. We can
> recognize what happened to us; it is similar to the experiment that suggested
> the futility of teaching psychology. When they made predictions about
> individual cases about which they had a little information (a brief and bland
> interview), Nisbett and Borgida’s students completely neglected the global
> results they had just learned. “Pallid” statistical information is routinely
> discarded when it is incompatible with one’s personal impressions of a case.
> In the competition with the inside view, the outside view doesn’t stand a
> chance.

> Bent Flyvbjerg, now at Oxford University, offered a forceful summary: The
> prevalent tendency to underweight or ignore distributional information is
> perhaps the major source of error in forecasting. Planners should therefore
> make every effort to frame the forecasting problem so as to facilitate
> utilizing all the distributional information that is available.

> for failing to allow for difficulties that they could not have anticipated—the
> unknown unknowns.

> call it theory-induced blindness: once you have accepted a theory and used it
> as a tool in your thinking, it is extraordinarily difficult to notice its
> flaws. If you come upon an observation that does not seem to fit the model,
> you assume that there must be a perfectly good explanation that you are
> somehow missing.

> You just like winning and dislike losing—and you almost certainly dislike
> losing more than you like winning.

> The slope of the function is steeper in the negative domain; the response to a
> loss is stronger than the response to a corresponding gain. This was the
> explanation of the endowment effect that Thaler had been searching for. And
> the first application of prospect theory to an economic puzzle now appears to
> have been a significant milestone in the development of behavioral economics.

> paper titled “Bad Is Stronger Than Good,” summarized the evidence as follows:
> “Bad emotions, bad parents, and bad feedback have more impact than good ones,
> and bad information is processed more thoroughly than good. The self is more
> motivated to avoid bad self-definitions than to pursue good ones. Bad
> impressions and bad stereotypes are quicker to form and more resistant to
> disconfirmation than good ones.” They cite John Gottman, the well-known expert
> in marital relations, who observed that the long-term success of a
> relationship depends far more on avoiding the negative than on seeking the
> positive. Gottman estimated that a stable relationship requires that good
> interactions outnumber bad interactions by at least 5 to 1.

> Loss aversion refers to the relative strength of two motives: we are driven
> more strongly to avoid losses than to achieve gains.

> Animals, including people, fight harder to prevent losses than to achieve
> gains. In the world of territorial animals, this principle explains the
> success of defenders. A biologist observed that “when a territory holder is
> challenged by a rival, the owner almost always wins the contest—usually within
> a matter of seconds.”

> von Neumann and Morgenstern introduced in 1944. They proved that any weighting
> of uncertain outcomes that is not strictly proportional to probability leads
> to inconsistencies and other disasters.

> My experience illustrates how terrorism works and why it is so effective: it
> induces an availability cascade.

> The psychology of high-prize lotteries is similar to the psychology of
> terrorism. The thrilling possibility of winning the big prize is shared by the
> community and reinforced by conversations at work and at home. Buying a ticket
> is immediately rewarded by pleasant fantasies, just as avoiding a bus was
> immediately rewarded by relief from fear. In both cases, the actual
> probability is inconsequential; only possibility matters. The original
> formulation of prospect theory included the argument that “highly unlikely
> events are either ignored or overweighted,” but it did not specify the
> conditions under which one or the other will occur, nor did it propose a
> psychological interpretation of it. My current view of decision weights has
> been strongly influenced by recent research on the role of emotions and
> vividness in decision making. Overweighting of unlikely outcomes is rooted in
> System 1 features that are familiar by now. Emotion and vividness influence
> fluency, availability, and judgments of probability—and thus account for our
> excessive response to the few rare events that we do not ignore.

> People overestimate the probabilities of unlikely events. People overweight
> unlikely events in their decisions. Although overestimation and overweighting
> are distinct phenomena, the same psychological mechanisms are involved in
> both: focused attention, confirmation bias, and cognitive ease.

> The common feature of these poignant stories is that they involve unusual
> events—and unusual events are easier than normal events to undo in
> imagination.

> An abnormal event attracts attention, and it also activates the idea of the
> event that would have been normal under the same circumstances.

> Losses are weighted about twice as much as gains in several contexts: choice
> between gambles, the endowment effect, and reactions to price changes. The
> loss-aversion coefficient is much higher in some situations. In particular,
> you may be more loss averse for aspects of your life that are more important
> than money, such as health. Furthermore, your reluctance to “sell” important
> endowments increases dramatically when doing so might make you responsible for
> an awful outcome.

> You can also take precautions that will inoculate you against regret. Perhaps
> the most useful is to be explicit about the anticipation of regret. If you can
> remember when things go badly that you considered the possibility of regret
> carefully before deciding, you are likely to experience less of it. You should
> also know that regret and hindsight bias will come together, so anything you
> can do to preclude hindsight is likely to be helpful. My personal
> hindsight-avoiding policy is to be either very thorough or completely casual
> when making a decision with long-term consequences. Hindsight is worse when
> you think a little, just enough to tell yourself later, “I almost made a
> better choice.”

> the emotion evoked by a word can “leak” into the final choice.

> But the experience was not actually ruined, only the memory of it.

> What we learn from the past is to maximize the qualities of our future
> memories, not necessarily of our future experience. This is the tyranny of the
> remembering self.

> Tastes and decisions are shaped by memories, and the memories can be wrong.

> story is about significant events and memorable moments, not about time
> passing.

> Duration neglect is normal in a story, and the ending often defines its
> character. The same core features appear in the rules of narratives and in the
> memories of colonoscopies, vacations, and films. This is how the remembering
> self works: it composes stories and keeps them for future reference.

> “You seem to be devoting your entire vacation to the construction of memories.
> Perhaps you should put away the camera and enjoy the moment, even if it is not
> very memorable?”

> Csikszentmihalyi calls flow—a state that some artists experience in their
> creative moments and that many other people achieve when enthralled by a film,
> a book, or a crossword puzzle: interruptions are not welcome in any of these
> situations.

> focusing illusion, which can be described in a single sentence: Nothing in
> life is as important as you think it is when you are thinking about it.

> We believe that duration is important, but our memory tells us it is not.

> Humans, unlike Econs, need help to make good decisions, and there are informed
> and unintrusive ways to provide that help.

> The way to block errors that originate in System 1 is simple in principle:
> recognize the signs that you are in a cognitive minefield, slow down, and ask
> for reinforcement from System 2.
